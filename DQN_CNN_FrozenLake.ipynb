{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文\n",
    "\n",
    "* [https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "\n",
    "# AIM\n",
    "\n",
    "簡単な問題でテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib nbagg\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import Chain, optimizers, Variable, serializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.5.2\n",
      "\n",
      "numpy 1.13.1\n",
      "matplotlib 2.0.2\n",
      "gym 0.9.2\n"
     ]
    }
   ],
   "source": [
    "from pkg_resources import get_distribution\n",
    "import platform\n",
    "print(\"python\", platform.python_version())\n",
    "print(\"\")\n",
    "libs = [\"numpy\", \"matplotlib\", \"gym\"]\n",
    "for lib in libs:\n",
    "    version = get_distribution(lib).version\n",
    "    print(lib, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-06 10:04:20,912] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space.n: 16\n",
      "action_space.n:  4\n",
      "\n",
      "obs:  0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "act:  0\n",
      "obs:  (4, 0.0, False, {'prob': 0.3333333333333333})\n",
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "print(\"observation_space.n:\", env.observation_space.n)\n",
    "print(\"action_space.n: \", env.action_space.n)\n",
    "print(\"\")\n",
    "\n",
    "obs = env.reset()\n",
    "print(\"obs: \", obs)\n",
    "env.render()\n",
    "print(\"\")\n",
    "\n",
    "act = env.action_space.sample()\n",
    "obs = env.step(act)\n",
    "print(\"act: \", act)\n",
    "print(\"obs: \", obs)\n",
    "env.render()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-06 11:05:07,613] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "epoch\tEPSILON\ttotal_reward\ttotal_step\telapsed_time\n",
      "100\t1\t0.0\t737\t0.03086709976196289[sec]\n",
      "200\t0.9520000000000053\t0.0\t1481\t1.8411839008331299[sec]\n",
      "300\t0.8797000000000132\t0.0\t2204\t2.562824010848999[sec]\n",
      "400\t0.8041000000000216\t0.0\t2960\t2.7136969566345215[sec]\n",
      "500\t0.7233000000000305\t0.0\t3768\t2.9716551303863525[sec]\n",
      "600\t0.6428000000000393\t0.0\t4573\t3.0141799449920654[sec]\n",
      "700\t0.5685000000000475\t0.0\t5316\t2.8471131324768066[sec]\n",
      "800\t0.49390000000005574\t0.0\t6062\t2.8832509517669678[sec]\n",
      "900\t0.4171000000000642\t0.0\t6830\t2.999462127685547[sec]\n",
      "1000\t0.33590000000007314\t0.0\t7642\t3.675112009048462[sec]\n",
      "1100\t0.26320000000008115\t0.0\t8369\t2.8910319805145264[sec]\n",
      "1200\t0.18940000000008927\t0.0\t9107\t2.999579906463623[sec]\n",
      "1300\t0.11310000000009603\t0.0\t9870\t3.245645046234131[sec]\n",
      "1400\t0.049900000000094216\t0.0\t10627\t3.165088176727295[sec]\n",
      "1500\t0.049900000000094216\t0.0\t11370\t3.1297669410705566[sec]\n",
      "1600\t0.049900000000094216\t0.0\t12084\t3.0102670192718506[sec]\n",
      "1700\t0.049900000000094216\t0.0\t12871\t3.310089111328125[sec]\n",
      "1800\t0.049900000000094216\t0.0\t13627\t3.3116440773010254[sec]\n",
      "1900\t0.049900000000094216\t0.0\t14400\t3.2992258071899414[sec]\n",
      "2000\t0.049900000000094216\t0.0\t15259\t3.580885887145996[sec]\n",
      "2100\t0.049900000000094216\t0.0\t16000\t3.6249680519104004[sec]\n",
      "2200\t0.049900000000094216\t0.0\t16744\t3.667710065841675[sec]\n",
      "2300\t0.049900000000094216\t0.0\t17625\t4.227936029434204[sec]\n",
      "2400\t0.049900000000094216\t0.0\t18359\t3.596604108810425[sec]\n",
      "2500\t0.049900000000094216\t0.0\t19145\t3.7638421058654785[sec]\n",
      "2600\t0.049900000000094216\t0.0\t19868\t3.5054330825805664[sec]\n",
      "2700\t0.049900000000094216\t0.0\t20662\t3.853968858718872[sec]\n",
      "2800\t0.049900000000094216\t0.0\t21506\t3.909273147583008[sec]\n",
      "2900\t0.049900000000094216\t0.0\t22150\t3.346144914627075[sec]\n",
      "3000\t0.049900000000094216\t0.0\t22953\t3.875074863433838[sec]\n"
     ]
    }
   ],
   "source": [
    "# 環境\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "# CNNクラス定義\n",
    "class CNN(Chain):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__(\n",
    "            xc = L.Convolution2D(None, 8, (1, 1)),\n",
    "            ch = L.Linear(32, 10),\n",
    "            hy = L.Linear(10, 4)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, t=None, train=False):\n",
    "        x = Variable(x)\n",
    "        if train:\n",
    "            t = Variable(t)\n",
    "        h = F.max_pooling_2d(F.relu(self.xc(x)), 2)\n",
    "        h = F.relu(self.ch(h))\n",
    "        y = F.softmax(self.hy(h))\n",
    "        if train:\n",
    "            return F.softmax_cross_entropy(y, t)\n",
    "        else:\n",
    "            return y\n",
    "        \n",
    "    def reset(self):\n",
    "        self.zerograds()\n",
    "        \n",
    "# 画像変換関数定義\n",
    "def convert(obs):\n",
    "    tmp = np.zeros(16)\n",
    "    tmp[obs] = 1\n",
    "    obs = np.array(tmp, dtype=\"float32\")\n",
    "    obs = obs.reshape(4,4)\n",
    "    return obs\n",
    "\n",
    "# モデル\n",
    "Q = CNN() # 近似Q関数\n",
    "#serializers.load_npz(\"./******.npz\", q) # 重みファイル読み込み\n",
    "Q_ast = copy.deepcopy(Q)\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(Q)\n",
    "\n",
    "# 定数\n",
    "EPOCH_NUM = 3000 # エポック数\n",
    "MEMORY_SIZE = 1000 # メモリサイズいくつで学習を開始するか\n",
    "BATCH_SIZE = 200 # バッチサイズ\n",
    "EPSILON = 1 # ε-greedy法\n",
    "EPSILON_DECREASE = 0.0001 # εの減少値\n",
    "EPSILON_MIN = 0.05 # εの下限\n",
    "START_REDUCE_EPSILON = 1000 # εを減少させるステップ数\n",
    "TRAIN_FREQ = 10 # Q関数の学習間隔\n",
    "UPDATE_TARGET_Q_FREQ = 20 # Q関数の更新間隔\n",
    "GAMMA = 0.99\n",
    "\n",
    "total_step = 0 # 総ステップ（行動）数\n",
    "memory = [] # メモリ\n",
    "total_rewards = np.zeros(EPOCH_NUM) # 累積報酬記録用リスト\n",
    "\n",
    "# 学習開始\n",
    "print(\"Train\")\n",
    "print(\"\\t\".join([\"epoch\", \"EPSILON\", \"total_reward\", \"total_step\", \"elapsed_time\"]))\n",
    "start = time.time()\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    pobs = env.reset() # 環境初期化\n",
    "    pobs = convert(pobs)\n",
    "    converted_height, converted_width = pobs.shape[0], pobs.shape[1] # 後で利用するので、変換後のサイズを取得\n",
    "    done = False # ゲーム終了フラグ\n",
    "    total_reward = 0 # 累積報酬\n",
    "    while not done:\n",
    "        # 行動選択\n",
    "        pact = env.action_space.sample()\n",
    "        if np.random.rand() > EPSILON: # ε-greedy法\n",
    "            y = Q(pobs.reshape((1, 1, converted_height, converted_width))) # 最適な行動を予測 # batchsize, channel, height, width\n",
    "        # 行動\n",
    "        obs, reward, done, _ = env.step(pact)\n",
    "        obs = convert(obs)\n",
    "        # メモリに蓄積\n",
    "        memory.append((pobs, pact, reward, obs, done)) # 変換済みの行動前状態ベクトル、未変換の行動ラベル、報酬、変換済みの行動後状態ベクトル、ゲーム終了フラグ\n",
    "        if len(memory) > MEMORY_SIZE: # メモリサイズを超えていれば消していく\n",
    "            memory.pop(0)\n",
    "        # 学習\n",
    "        if len(memory) == MEMORY_SIZE: # メモリサイズ分溜まっていれば学習\n",
    "            # 経験リプレイ\n",
    "            if total_step % TRAIN_FREQ == 0:\n",
    "                np.random.shuffle(memory)\n",
    "                memory_idx = range(len(memory))\n",
    "                for i in memory_idx[::BATCH_SIZE]:\n",
    "                    batch = np.array(memory[i:i+BATCH_SIZE]) # 経験ミニバッチ\n",
    "                    pobss = np.array(batch[:,0].tolist(), dtype=\"float32\").reshape((BATCH_SIZE, 1, converted_height, converted_width))\n",
    "                    pacts = np.array(batch[:,1].tolist(), dtype=\"int32\")\n",
    "                    rewards = np.array(batch[:,2].tolist(), dtype=\"int32\")\n",
    "                    obss = np.array(batch[:,3].tolist(), dtype=\"float32\").reshape((BATCH_SIZE, 1, converted_height, converted_width))\n",
    "                    dones = np.array(batch[:,4].tolist(), dtype=\"bool\")\n",
    "                    # set y\n",
    "                    q = Q(pobss)\n",
    "                    maxq = list(map(np.max, Q_ast(obss).data)) # maxQ\n",
    "                    target = copy.deepcopy(q.data)\n",
    "                    for j in range(BATCH_SIZE):\n",
    "                        target[j, pacts[j]] = rewards[j]+GAMMA*maxq[j]*(not dones[j])\n",
    "                    # Perform a gradient descent step\n",
    "                    Q.reset()\n",
    "                    loss = F.mean_squared_error(q, Variable(target))\n",
    "                    loss.backward()\n",
    "                    optimizer.update()\n",
    "            # Q関数の更新\n",
    "            if total_step % UPDATE_TARGET_Q_FREQ == 0:\n",
    "                Q_ast = copy.deepcopy(Q)\n",
    "        # εの減少\n",
    "        if EPSILON > EPSILON_MIN:\n",
    "            if total_step > START_REDUCE_EPSILON:\n",
    "                EPSILON -= EPSILON_DECREASE\n",
    "        # 次の行動へ\n",
    "        total_reward += reward\n",
    "        total_step += 1\n",
    "        pobs = obs\n",
    "    total_rewards[epoch] = total_reward # 累積報酬を記録\n",
    "    #serializers.save_npz(\"./******.npz\", q) # 重みファイル出力\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        elapsed_time = time.time()-start\n",
    "        print(\"\\t\".join(map(str,[epoch+1, EPSILON, total_reward, total_step, str(elapsed_time)+\"[sec]\"]))) # ログ出力\n",
    "        start = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
