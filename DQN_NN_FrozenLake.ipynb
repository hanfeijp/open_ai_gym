{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 論文\n",
    "\n",
    "* [https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf)\n",
    "\n",
    "# AIM\n",
    "\n",
    "簡単な問題でテスト"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "%matplotlib nbagg\n",
    "import chainer\n",
    "import chainer.links as L\n",
    "import chainer.functions as F\n",
    "from chainer import Chain, optimizers, Variable, serializers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python 3.5.2\n",
      "\n",
      "numpy 1.13.1\n",
      "matplotlib 2.0.2\n",
      "gym 0.9.2\n"
     ]
    }
   ],
   "source": [
    "from pkg_resources import get_distribution\n",
    "import platform\n",
    "print(\"python\", platform.python_version())\n",
    "print(\"\")\n",
    "libs = [\"numpy\", \"matplotlib\", \"gym\"]\n",
    "for lib in libs:\n",
    "    version = get_distribution(lib).version\n",
    "    print(lib, version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-06 19:49:20,648] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation_space.n: 16\n",
      "action_space.n:  4\n",
      "\n",
      "obs:  0\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "act:  0\n",
      "obs:  (0, 0.0, False, {'prob': 0.3333333333333333})\n",
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"FrozenLake-v0\")\n",
    "print(\"observation_space.n:\", env.observation_space.n)\n",
    "print(\"action_space.n: \", env.action_space.n)\n",
    "print(\"\")\n",
    "\n",
    "obs = env.reset()\n",
    "print(\"obs: \", obs)\n",
    "env.render()\n",
    "print(\"\")\n",
    "\n",
    "act = env.action_space.sample()\n",
    "obs = env.step(act)\n",
    "print(\"act: \", act)\n",
    "print(\"obs: \", obs)\n",
    "env.render()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2017-09-06 19:55:40,997] Making new env: FrozenLake-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train\n",
      "epoch\tEPSILON\ttotal_reward\ttotal_step\telapsed_time\n",
      "100\t1\t0.0\t780\t0.030592918395996094[sec]\n",
      "200\t0.9492000000000056\t0.0\t1509\t1.4598109722137451[sec]\n",
      "300\t0.8751000000000138\t0.0\t2250\t2.0775279998779297[sec]\n",
      "400\t0.800600000000022\t0.0\t2995\t2.1633191108703613[sec]\n",
      "500\t0.7240000000000304\t0.0\t3761\t2.2294631004333496[sec]\n",
      "600\t0.6506000000000385\t0.0\t4495\t2.1513779163360596[sec]\n",
      "700\t0.5790000000000464\t0.0\t5211\t2.132861852645874[sec]\n",
      "800\t0.5074000000000543\t0.0\t5927\t2.1199989318847656[sec]\n",
      "900\t0.4400000000000617\t0.0\t6601\t2.046755790710449[sec]\n",
      "1000\t0.36660000000006976\t0.0\t7335\t2.219085931777954[sec]\n",
      "1100\t0.2882000000000784\t0.0\t8119\t2.419045925140381[sec]\n",
      "1200\t0.21480000000008648\t0.0\t8853\t2.3922200202941895[sec]\n",
      "1300\t0.1446000000000942\t0.0\t9555\t2.2295758724212646[sec]\n",
      "1400\t0.07180000000009484\t0.0\t10283\t2.306668996810913[sec]\n",
      "1500\t0.049900000000094216\t0.0\t11009\t2.3054778575897217[sec]\n",
      "1600\t0.049900000000094216\t0.0\t11953\t3.032641887664795[sec]\n",
      "1700\t0.049900000000094216\t0.0\t12613\t2.1288070678710938[sec]\n",
      "1800\t0.049900000000094216\t0.0\t13493\t3.39973783493042[sec]\n",
      "1900\t0.049900000000094216\t0.0\t14239\t2.5638461112976074[sec]\n",
      "2000\t0.049900000000094216\t0.0\t14961\t2.3535590171813965[sec]\n",
      "2100\t0.049900000000094216\t0.0\t15776\t3.022472858428955[sec]\n",
      "2200\t0.049900000000094216\t0.0\t16451\t2.484477996826172[sec]\n",
      "2300\t0.049900000000094216\t0.0\t17145\t2.301483154296875[sec]\n",
      "2400\t0.049900000000094216\t0.0\t17824\t2.5166919231414795[sec]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-76a9989a4d42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m                     \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"bool\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                     \u001b[0;31m# set y\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m                     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpobss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m                     \u001b[0mmaxq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_ast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# maxQ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0mmaxq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"float32\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-76a9989a4d42>\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x, t, train)\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.2/envs/study_open_ai_gym/lib/python3.5/site-packages/chainer/functions/activation/relu.py\u001b[0m in \u001b[0;36mrelu\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m     \"\"\"\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.pyenv/versions/3.5.2/envs/study_open_ai_gym/lib/python3.5/site-packages/chainer/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mconfiguration\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_data_type_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mhooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_function_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.2/envs/study_open_ai_gym/lib/python3.5/site-packages/chainer/function.py\u001b[0m in \u001b[0;36m_check_data_type_forward\u001b[0;34m(self, in_data)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_data_type_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m         \u001b[0min_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_light_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtype_check\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlight_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/3.5.2/envs/study_open_ai_gym/lib/python3.5/site-packages/chainer/utils/type_check.py\u001b[0m in \u001b[0;36mget_light_types\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mget_light_types\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 環境\n",
    "env = gym.make(\"FrozenLake-v0\")\n",
    "\n",
    "# NNクラス定義\n",
    "class NN(Chain):\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__(\n",
    "            xc = L.Linear(16, 100),\n",
    "            ch = L.Linear(100, 100),\n",
    "            hy = L.Linear(100, 4)\n",
    "        )\n",
    "        \n",
    "    def __call__(self, x, t=None, train=False):\n",
    "        x = Variable(x)\n",
    "        if train:\n",
    "            t = Variable(t)\n",
    "        h = F.relu(self.xc(x))\n",
    "        h = F.relu(self.ch(h))\n",
    "        y = F.softmax(self.hy(h))\n",
    "        if train:\n",
    "            return F.softmax_cross_entropy(y, t)\n",
    "        else:\n",
    "            return y\n",
    "        \n",
    "    def reset(self):\n",
    "        self.zerograds()\n",
    "        \n",
    "# 状態変換関数定義\n",
    "def convert(obs):\n",
    "    tmp = np.zeros(16)\n",
    "    tmp[obs] = 1\n",
    "    obs = np.array(tmp, dtype=\"float32\")\n",
    "    return obs\n",
    "\n",
    "# モデル\n",
    "Q = NN() # 近似Q関数\n",
    "#serializers.load_npz(\"./******.npz\", q) # 重みファイル読み込み\n",
    "Q_ast = copy.deepcopy(Q)\n",
    "optimizer = optimizers.Adam()\n",
    "optimizer.setup(Q)\n",
    "\n",
    "# 定数\n",
    "EPOCH_NUM = 3000 # エポック数\n",
    "MEMORY_SIZE = 1000 # メモリサイズいくつで学習を開始するか\n",
    "BATCH_SIZE = 200 # バッチサイズ\n",
    "EPSILON = 1 # ε-greedy法\n",
    "EPSILON_DECREASE = 0.0001 # εの減少値\n",
    "EPSILON_MIN = 0.05 # εの下限\n",
    "START_REDUCE_EPSILON = 1000 # εを減少させるステップ数\n",
    "TRAIN_FREQ = 10 # Q関数の学習間隔\n",
    "UPDATE_TARGET_Q_FREQ = 20 # Q関数の更新間隔\n",
    "GAMMA = 0.99\n",
    "\n",
    "total_step = 0 # 総ステップ（行動）数\n",
    "memory = [] # メモリ\n",
    "total_rewards = np.zeros(EPOCH_NUM) # 累積報酬記録用リスト\n",
    "\n",
    "# 学習開始\n",
    "print(\"Train\")\n",
    "print(\"\\t\".join([\"epoch\", \"EPSILON\", \"total_reward\", \"total_step\", \"elapsed_time\"]))\n",
    "start = time.time()\n",
    "for epoch in range(EPOCH_NUM):\n",
    "    pobs = env.reset() # 環境初期化\n",
    "    pobs = convert(pobs)\n",
    "    done = False # ゲーム終了フラグ\n",
    "    total_reward = 0 # 累積報酬\n",
    "    while not done:\n",
    "        # 行動選択\n",
    "        pact = env.action_space.sample()\n",
    "        if np.random.rand() > EPSILON: # ε-greedy法\n",
    "            y = Q(pobs.reshape((1, 16))) # 最適な行動を予測 # batchsize, channel, height, width\n",
    "        # 行動\n",
    "        obs, reward, done, _ = env.step(pact)\n",
    "        obs = convert(obs)\n",
    "        # メモリに蓄積\n",
    "        memory.append((pobs, pact, reward, obs, done)) # 変換済みの行動前状態ベクトル、未変換の行動ラベル、報酬、変換済みの行動後状態ベクトル、ゲーム終了フラグ\n",
    "        if len(memory) > MEMORY_SIZE: # メモリサイズを超えていれば消していく\n",
    "            memory.pop(0)\n",
    "        # 学習\n",
    "        if len(memory) == MEMORY_SIZE: # メモリサイズ分溜まっていれば学習\n",
    "            # 経験リプレイ\n",
    "            if total_step % TRAIN_FREQ == 0:\n",
    "                np.random.shuffle(memory)\n",
    "                memory_idx = range(len(memory))\n",
    "                for i in memory_idx[::BATCH_SIZE]:\n",
    "                    batch = np.array(memory[i:i+BATCH_SIZE]) # 経験ミニバッチ\n",
    "                    pobss = np.array(batch[:,0].tolist(), dtype=\"float32\").reshape((BATCH_SIZE, 16))\n",
    "                    pacts = np.array(batch[:,1].tolist(), dtype=\"int32\")\n",
    "                    rewards = np.array(batch[:,2].tolist(), dtype=\"int32\")\n",
    "                    obss = np.array(batch[:,3].tolist(), dtype=\"float32\").reshape((BATCH_SIZE, 16))\n",
    "                    dones = np.array(batch[:,4].tolist(), dtype=\"bool\")\n",
    "                    # set y\n",
    "                    q = Q(pobss)\n",
    "                    maxq = list(map(np.max, Q_ast(obss).data)) # maxQ\n",
    "                    maxq = np.asanyarray(maxq, dtype=\"float32\")\n",
    "                    target = copy.deepcopy(q.data)\n",
    "                    target = np.asanyarray(target, dtype=\"float32\")\n",
    "                    for j in range(BATCH_SIZE):\n",
    "                        target[j, pacts[j]] = rewards[j]+GAMMA*maxq[j]*(not dones[j])\n",
    "                    # Perform a gradient descent step\n",
    "                    Q.reset()\n",
    "                    loss = F.mean_squared_error(q, Variable(target))\n",
    "                    loss.backward()\n",
    "                    optimizer.update()\n",
    "            # Q関数の更新\n",
    "            if total_step % UPDATE_TARGET_Q_FREQ == 0:\n",
    "                Q_ast = copy.deepcopy(Q)\n",
    "        # εの減少\n",
    "        if EPSILON > EPSILON_MIN:\n",
    "            if total_step > START_REDUCE_EPSILON:\n",
    "                EPSILON -= EPSILON_DECREASE\n",
    "        # 次の行動へ\n",
    "        total_reward += reward\n",
    "        total_step += 1\n",
    "        pobs = obs\n",
    "    total_rewards[epoch] = total_reward # 累積報酬を記録\n",
    "    #serializers.save_npz(\"./******.npz\", q) # 重みファイル出力\n",
    "    if (epoch+1) % 100 == 0:\n",
    "        elapsed_time = time.time()-start\n",
    "        print(\"\\t\".join(map(str,[epoch+1, EPSILON, total_reward, total_step, str(elapsed_time)+\"[sec]\"]))) # ログ出力\n",
    "        start = time.time()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
